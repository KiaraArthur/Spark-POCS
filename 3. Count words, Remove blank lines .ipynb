{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.4.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 2.7.10 (default, May 28 2015 16:44:52)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# Configure the environment\n",
    "if 'SPARK_HOME' not in os.environ:\n",
    "    os.environ['SPARK_HOME'] = 'C:\\spark-1.4.1-bin-hadoop2.4\\spark-1.4.1-bin-hadoop2.4'\n",
    "\n",
    "# Create a variable for our root path\n",
    "SPARK_HOME = os.environ['SPARK_HOME']\n",
    "\n",
    "# Add the PySpark/py4j to the Python Path\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\", \"lib/py4j-0.8.2.1-src.zip\"))\n",
    "sys.path.insert(0, os.path.join(SPARK_HOME, \"python\"))\n",
    "execfile(os.path.join(SPARK_HOME, 'python/pyspark/shell.py'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 41\n",
      "all: 1\n",
      "help: 1\n",
      "when: 1\n",
      "automated: 1\n",
      "Hadoop: 4\n",
      "\"local\": 1\n",
      "including: 3\n",
      "computation: 1\n",
      "[\"Third: 1\n",
      "file: 1\n",
      "high-level: 1\n",
      "find: 1\n",
      "web: 1\n",
      "Shell: 2\n",
      "cluster: 2\n",
      "how: 2\n",
      "using:: 1\n",
      "Big: 1\n",
      "guidance: 3\n",
      "run:: 1\n",
      "Scala,: 1\n",
      "Running: 1\n",
      "should: 2\n",
      "environment: 1\n",
      "to: 14\n",
      "only: 1\n",
      "given.: 1\n",
      "rich: 1\n",
      "directory.: 1\n",
      "Apache: 1\n",
      "Interactive: 2\n",
      "sc.parallelize(range(1000)).count(): 1\n",
      "Building: 1\n",
      "do: 2\n",
      "guide,: 1\n",
      "return: 2\n",
      "graphs: 1\n",
      "Programs: 1\n",
      "Many: 1\n",
      "Try: 1\n",
      "built,: 1\n",
      "\"yarn-client\": 1\n",
      "YARN,: 1\n",
      "not: 1\n",
      "using: 2\n",
      "Example: 1\n",
      "scala>: 1\n",
      "Once: 1\n",
      "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).: 1\n",
      "Because: 1\n",
      "cluster.: 1\n",
      "name: 1\n",
      "Testing: 1\n",
      "Streaming: 1\n",
      "./bin/pyspark: 1\n",
      "SQL: 2\n",
      "through: 1\n",
      "GraphX: 1\n",
      "them,: 1\n",
      "[run: 1\n",
      "\"yarn-cluster\": 1\n",
      "the: 21\n",
      "abbreviated: 1\n",
      "set: 2\n",
      "[project: 2\n",
      "Scala: 2\n",
      "##: 8\n",
      "thread,: 1\n",
      "library: 1\n",
      "see: 1\n",
      "examples: 2\n",
      "MASTER: 1\n",
      "runs.: 1\n",
      "[Apache: 1\n",
      "Pi: 1\n",
      "instructions.: 1\n",
      "More: 1\n",
      "Python,: 2\n",
      "#: 1\n",
      "processing,: 2\n",
      "for: 11\n",
      "its: 1\n",
      "version: 1\n",
      "wiki](https://cwiki.apache.org/confluence/display/SPARK).: 1\n",
      "provides: 1\n",
      "print: 1\n",
      "Configuration: 1\n",
      "supports: 2\n",
      "command,: 2\n",
      "[params]`.: 1\n",
      "refer: 2\n",
      "available: 1\n",
      "core: 1\n",
      "run: 7\n",
      "./bin/run-example: 2\n",
      "Versions: 1\n",
      "This: 2\n",
      "Hadoop,: 2\n",
      "Documentation: 1\n",
      "use: 3\n",
      "downloaded: 1\n",
      "distributions.: 1\n",
      "Spark.: 1\n",
      "example:: 1\n",
      "`examples`: 2\n",
      "-DskipTests: 1\n",
      "Maven](http://maven.apache.org/).: 1\n",
      "[\"Building: 1\n",
      "works: 1\n",
      "package: 1\n",
      "of: 5\n",
      "changed: 1\n",
      "programming: 1\n",
      "optimized: 1\n",
      "against: 1\n",
      "site,: 1\n",
      "graph: 1\n",
      "For: 2\n",
      "or: 3\n",
      "mvn: 1\n",
      "learning,: 1\n",
      "and: 10\n",
      "contains: 1\n",
      "stream: 1\n",
      "can: 6\n",
      "overview: 1\n",
      "package.): 1\n",
      "prefer: 1\n",
      "one: 2\n",
      "documentation,: 1\n",
      "Data.: 1\n",
      "(You: 1\n",
      "Online: 1\n",
      "tools: 1\n",
      "your: 1\n",
      "threads.: 1\n",
      "Tests: 1\n",
      "fast: 1\n",
      "from: 1\n",
      "package.: 1\n",
      "APIs: 1\n",
      "SparkPi: 2\n",
      "structured: 1\n",
      "system: 1\n",
      "submit: 1\n",
      "systems.: 1\n",
      "start: 1\n",
      "params: 1\n",
      "Hadoop-supported: 1\n",
      "way: 1\n",
      "basic: 1\n",
      "README: 1\n",
      "<http://spark.apache.org/>: 1\n",
      "It: 2\n",
      "Spark](#building-spark).: 1\n",
      "engine: 1\n",
      "building: 3\n",
      "configure: 1\n",
      "on: 6\n",
      "Note: 1\n",
      "N: 1\n",
      "usage: 1\n",
      "\"local[N]\": 1\n",
      ">>>: 1\n",
      "particular: 3\n",
      "instance:: 1\n",
      "./bin/spark-shell: 1\n",
      "be: 2\n",
      "general: 2\n",
      "with: 4\n",
      "easiest: 1\n",
      "protocols: 1\n",
      "must: 1\n",
      "And: 1\n",
      "also: 5\n",
      "versions: 1\n",
      "this: 1\n",
      "project: 1\n",
      "setup: 1\n",
      "page](http://spark.apache.org/documentation.html): 1\n",
      "shell:: 2\n",
      "will: 1\n",
      "See: 1\n",
      "`./bin/run-example: 1\n",
      "guide](http://spark.apache.org/docs/latest/configuration.html): 1\n",
      "following: 2\n",
      "locally: 2\n",
      "distribution: 1\n",
      "example: 3\n",
      "are: 1\n",
      "detailed: 2\n",
      "tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).: 1\n",
      "mesos://: 1\n",
      "Please: 3\n",
      "computing: 1\n",
      "URL,: 1\n",
      "is: 6\n",
      "in: 5\n",
      "higher-level: 1\n",
      "tests: 1\n",
      "1000:: 2\n",
      "an: 3\n",
      "sample: 1\n",
      "To: 2\n",
      "at: 2\n",
      "have: 1\n",
      "1000).count(): 1\n",
      "[\"Specifying: 1\n",
      "Party: 1\n",
      "[building: 1\n",
      "You: 3\n",
      "if: 4\n",
      "Spark: 14\n",
      "different: 1\n",
      "MASTER=spark://host:7077: 1\n",
      "no: 1\n",
      "programs,: 1\n",
      "Java,: 1\n",
      "that: 3\n",
      "storage: 1\n",
      "MLlib: 1\n",
      "same: 1\n",
      "machine: 1\n",
      "application: 1\n",
      "need: 1\n",
      "other: 1\n",
      "analysis.: 1\n",
      "build: 3\n",
      "which: 2\n",
      "online: 1\n",
      "you: 4\n",
      "several: 1\n",
      "A: 1\n",
      "distribution.: 1\n",
      "About: 1\n",
      "HDFS: 1\n",
      "[Configuration: 1\n",
      "sc.parallelize(1: 1\n",
      "locally.: 1\n",
      "Hive: 2\n",
      "running: 1\n",
      "uses: 1\n",
      "a: 9\n",
      "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version): 1\n",
      "variable: 1\n",
      "The: 1\n",
      "data: 2\n",
      "class: 2\n",
      "built: 1\n",
      "Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html): 1\n",
      "Thriftserver: 1\n",
      "processing.: 1\n",
      "programs: 2\n",
      "documentation: 3\n",
      "pre-built: 1\n",
      "Alternatively,: 1\n",
      "Python: 2\n",
      "./dev/run-tests: 1\n",
      "comes: 1\n",
      "clean: 1\n",
      "<class>: 1\n",
      "spark://: 1\n",
      "first: 1\n",
      "requires: 1\n",
      "talk: 1\n",
      "latest: 1\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(os.path.join(SPARK_HOME, \"README.md\"), 1)\n",
    "linesnew = lines.filter( lambda x: len(x) > 0 )\n",
    "counts = linesnew.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "   print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'# Apache Spark',\n",
       " u'Spark is a fast and general cluster computing system for Big Data. It provides',\n",
       " u'high-level APIs in Scala, Java, and Python, and an optimized engine that',\n",
       " u'supports general computation graphs for data analysis. It also supports a',\n",
       " u'rich set of higher-level tools including Spark SQL for SQL and structured',\n",
       " u'data processing, MLlib for machine learning, GraphX for graph processing,',\n",
       " u'and Spark Streaming for stream processing.',\n",
       " u'<http://spark.apache.org/>',\n",
       " u'## Online Documentation',\n",
       " u'You can find the latest Spark documentation, including a programming',\n",
       " u'guide, on the [project web page](http://spark.apache.org/documentation.html)',\n",
       " u'and [project wiki](https://cwiki.apache.org/confluence/display/SPARK).',\n",
       " u'This README file only contains basic setup instructions.',\n",
       " u'## Building Spark',\n",
       " u'Spark is built using [Apache Maven](http://maven.apache.org/).',\n",
       " u'To build Spark and its example programs, run:',\n",
       " u'    mvn -DskipTests clean package',\n",
       " u'(You do not need to do this if you downloaded a pre-built package.)',\n",
       " u'More detailed documentation is available from the project site, at',\n",
       " u'[\"Building Spark\"](http://spark.apache.org/docs/latest/building-spark.html).',\n",
       " u'## Interactive Scala Shell',\n",
       " u'The easiest way to start using Spark is through the Scala shell:',\n",
       " u'    ./bin/spark-shell',\n",
       " u'Try the following command, which should return 1000:',\n",
       " u'    scala> sc.parallelize(1 to 1000).count()',\n",
       " u'## Interactive Python Shell',\n",
       " u'Alternatively, if you prefer Python, you can use the Python shell:',\n",
       " u'    ./bin/pyspark',\n",
       " u'    ',\n",
       " u'And run the following command, which should also return 1000:',\n",
       " u'    >>> sc.parallelize(range(1000)).count()',\n",
       " u'## Example Programs',\n",
       " u'Spark also comes with several sample programs in the `examples` directory.',\n",
       " u'To run one of them, use `./bin/run-example <class> [params]`. For example:',\n",
       " u'    ./bin/run-example SparkPi',\n",
       " u'will run the Pi example locally.',\n",
       " u'You can set the MASTER environment variable when running examples to submit',\n",
       " u'examples to a cluster. This can be a mesos:// or spark:// URL, ',\n",
       " u'\"yarn-cluster\" or \"yarn-client\" to run on YARN, and \"local\" to run ',\n",
       " u'locally with one thread, or \"local[N]\" to run locally with N threads. You ',\n",
       " u'can also use an abbreviated class name if the class is in the `examples`',\n",
       " u'package. For instance:',\n",
       " u'    MASTER=spark://host:7077 ./bin/run-example SparkPi',\n",
       " u'Many of the example programs print usage help if no params are given.',\n",
       " u'## Running Tests',\n",
       " u'Testing first requires [building Spark](#building-spark). Once Spark is built, tests',\n",
       " u'can be run using:',\n",
       " u'    ./dev/run-tests',\n",
       " u'Please see the guidance on how to ',\n",
       " u'[run all automated tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).',\n",
       " u'## A Note About Hadoop Versions',\n",
       " u'Spark uses the Hadoop core library to talk to HDFS and other Hadoop-supported',\n",
       " u'storage systems. Because the protocols have changed in different versions of',\n",
       " u'Hadoop, you must build Spark against the same version that your cluster runs.',\n",
       " u'Please refer to the build documentation at',\n",
       " u'[\"Specifying the Hadoop Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version)',\n",
       " u'for detailed guidance on building for a particular distribution of Hadoop, including',\n",
       " u'building for particular Hive and Hive Thriftserver distributions. See also',\n",
       " u'[\"Third Party Hadoop Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html)',\n",
       " u'for guidance on building a Spark application that works with a particular',\n",
       " u'distribution.',\n",
       " u'## Configuration',\n",
       " u'Please refer to the [Configuration guide](http://spark.apache.org/docs/latest/configuration.html)',\n",
       " u'in the online documentation for an overview on how to configure Spark.']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linesnew.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 41\n",
      "all: 1\n",
      "help: 1\n",
      "when: 1\n",
      "automated: 1\n",
      "Hadoop: 4\n",
      "\"local\": 1\n",
      "including: 3\n",
      "computation: 1\n",
      "[\"Third: 1\n",
      "file: 1\n",
      "high-level: 1\n",
      "find: 1\n",
      "web: 1\n",
      "Shell: 2\n",
      "cluster: 2\n",
      "how: 2\n",
      "using:: 1\n",
      "Big: 1\n",
      "guidance: 3\n",
      "run:: 1\n",
      "Scala,: 1\n",
      "Running: 1\n",
      "should: 2\n",
      "environment: 1\n",
      "to: 14\n",
      "only: 1\n",
      "given.: 1\n",
      "rich: 1\n",
      "directory.: 1\n",
      "Apache: 1\n",
      "Interactive: 2\n",
      "sc.parallelize(range(1000)).count(): 1\n",
      "Building: 1\n",
      "do: 2\n",
      "guide,: 1\n",
      "return: 2\n",
      "graphs: 1\n",
      "Programs: 1\n",
      "Many: 1\n",
      "Try: 1\n",
      "built,: 1\n",
      "\"yarn-client\": 1\n",
      "YARN,: 1\n",
      "not: 1\n",
      "using: 2\n",
      "Example: 1\n",
      "scala>: 1\n",
      "Once: 1\n",
      "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).: 1\n",
      "Because: 1\n",
      "cluster.: 1\n",
      "name: 1\n",
      "Testing: 1\n",
      "Streaming: 1\n",
      "./bin/pyspark: 1\n",
      "SQL: 2\n",
      "through: 1\n",
      "GraphX: 1\n",
      "them,: 1\n",
      "[run: 1\n",
      "\"yarn-cluster\": 1\n",
      "the: 21\n",
      "abbreviated: 1\n",
      "set: 2\n",
      "[project: 2\n",
      "Scala: 2\n",
      "##: 8\n",
      "thread,: 1\n",
      "library: 1\n",
      "see: 1\n",
      "examples: 2\n",
      "MASTER: 1\n",
      "runs.: 1\n",
      "[Apache: 1\n",
      "Pi: 1\n",
      "instructions.: 1\n",
      "More: 1\n",
      "Python,: 2\n",
      "#: 1\n",
      "processing,: 2\n",
      "for: 11\n",
      "its: 1\n",
      "version: 1\n",
      "wiki](https://cwiki.apache.org/confluence/display/SPARK).: 1\n",
      "provides: 1\n",
      "print: 1\n",
      "Configuration: 1\n",
      "supports: 2\n",
      "command,: 2\n",
      "[params]`.: 1\n",
      "refer: 2\n",
      "available: 1\n",
      "core: 1\n",
      "run: 7\n",
      "./bin/run-example: 2\n",
      "Versions: 1\n",
      "This: 2\n",
      "Hadoop,: 2\n",
      "Documentation: 1\n",
      "use: 3\n",
      "downloaded: 1\n",
      "distributions.: 1\n",
      "Spark.: 1\n",
      "example:: 1\n",
      "`examples`: 2\n",
      "-DskipTests: 1\n",
      "Maven](http://maven.apache.org/).: 1\n",
      "[\"Building: 1\n",
      "works: 1\n",
      "package: 1\n",
      "of: 5\n",
      "changed: 1\n",
      "programming: 1\n",
      "optimized: 1\n",
      "against: 1\n",
      "site,: 1\n",
      "graph: 1\n",
      "For: 2\n",
      "or: 3\n",
      "mvn: 1\n",
      "learning,: 1\n",
      "and: 10\n",
      "contains: 1\n",
      "stream: 1\n",
      "can: 6\n",
      "overview: 1\n",
      "package.): 1\n",
      "prefer: 1\n",
      "one: 2\n",
      "documentation,: 1\n",
      "Data.: 1\n",
      "(You: 1\n",
      "Online: 1\n",
      "tools: 1\n",
      "your: 1\n",
      "threads.: 1\n",
      "Tests: 1\n",
      "fast: 1\n",
      "from: 1\n",
      "package.: 1\n",
      "APIs: 1\n",
      "SparkPi: 2\n",
      "structured: 1\n",
      "system: 1\n",
      "submit: 1\n",
      "systems.: 1\n",
      "start: 1\n",
      "params: 1\n",
      "Hadoop-supported: 1\n",
      "way: 1\n",
      "basic: 1\n",
      "README: 1\n",
      "<http://spark.apache.org/>: 1\n",
      "It: 2\n",
      "Spark](#building-spark).: 1\n",
      "engine: 1\n",
      "building: 3\n",
      "configure: 1\n",
      "on: 6\n",
      "Note: 1\n",
      "N: 1\n",
      "usage: 1\n",
      "\"local[N]\": 1\n",
      ">>>: 1\n",
      "particular: 3\n",
      "instance:: 1\n",
      "./bin/spark-shell: 1\n",
      "be: 2\n",
      "general: 2\n",
      "with: 4\n",
      "easiest: 1\n",
      "protocols: 1\n",
      "must: 1\n",
      "And: 1\n",
      "also: 5\n",
      "versions: 1\n",
      "this: 1\n",
      "project: 1\n",
      "setup: 1\n",
      "page](http://spark.apache.org/documentation.html): 1\n",
      "shell:: 2\n",
      "will: 1\n",
      "See: 1\n",
      "`./bin/run-example: 1\n",
      "guide](http://spark.apache.org/docs/latest/configuration.html): 1\n",
      "following: 2\n",
      "locally: 2\n",
      "distribution: 1\n",
      "example: 3\n",
      "are: 1\n",
      "detailed: 2\n",
      "tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).: 1\n",
      "mesos://: 1\n",
      "Please: 3\n",
      "computing: 1\n",
      "URL,: 1\n",
      "is: 6\n",
      "in: 5\n",
      "higher-level: 1\n",
      "tests: 1\n",
      "1000:: 2\n",
      "an: 3\n",
      "sample: 1\n",
      "To: 2\n",
      "at: 2\n",
      "have: 1\n",
      "1000).count(): 1\n",
      "[\"Specifying: 1\n",
      "Party: 1\n",
      "[building: 1\n",
      "You: 3\n",
      "if: 4\n",
      "Spark: 14\n",
      "different: 1\n",
      "MASTER=spark://host:7077: 1\n",
      "no: 1\n",
      "programs,: 1\n",
      "Java,: 1\n",
      "that: 3\n",
      "storage: 1\n",
      "MLlib: 1\n",
      "same: 1\n",
      "machine: 1\n",
      "application: 1\n",
      "need: 1\n",
      "other: 1\n",
      "analysis.: 1\n",
      "build: 3\n",
      "which: 2\n",
      "online: 1\n",
      "you: 4\n",
      "several: 1\n",
      "A: 1\n",
      "distribution.: 1\n",
      "About: 1\n",
      "HDFS: 1\n",
      "[Configuration: 1\n",
      "sc.parallelize(1: 1\n",
      "locally.: 1\n",
      "Hive: 2\n",
      "running: 1\n",
      "uses: 1\n",
      "a: 9\n",
      "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version): 1\n",
      "variable: 1\n",
      "The: 1\n",
      "data: 2\n",
      "class: 2\n",
      "built: 1\n",
      "Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html): 1\n",
      "Thriftserver: 1\n",
      "processing.: 1\n",
      "programs: 2\n",
      "documentation: 3\n",
      "pre-built: 1\n",
      "Alternatively,: 1\n",
      "Python: 2\n",
      "./dev/run-tests: 1\n",
      "comes: 1\n",
      "clean: 1\n",
      "<class>: 1\n",
      "spark://: 1\n",
      "first: 1\n",
      "requires: 1\n",
      "talk: 1\n",
      "latest: 1\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(os.path.join(SPARK_HOME, \"README.md\"), 1)\n",
    "linesnew = lines.filter( lambda x: len(x) > 1 )\n",
    "counts = linesnew.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "   print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ": 41\n",
      "all: 1\n",
      "help: 1\n",
      "when: 1\n",
      "automated: 1\n",
      "Hadoop: 4\n",
      "\"local\": 1\n",
      "including: 3\n",
      "computation: 1\n",
      "[\"Third: 1\n",
      "file: 1\n",
      "high-level: 1\n",
      "find: 1\n",
      "web: 1\n",
      "Shell: 2\n",
      "cluster: 2\n",
      "how: 2\n",
      "using:: 1\n",
      "Big: 1\n",
      "guidance: 3\n",
      "run:: 1\n",
      "Scala,: 1\n",
      "Running: 1\n",
      "should: 2\n",
      "environment: 1\n",
      "to: 14\n",
      "only: 1\n",
      "given.: 1\n",
      "rich: 1\n",
      "directory.: 1\n",
      "Apache: 1\n",
      "Interactive: 2\n",
      "sc.parallelize(range(1000)).count(): 1\n",
      "Building: 1\n",
      "do: 2\n",
      "guide,: 1\n",
      "return: 2\n",
      "graphs: 1\n",
      "Programs: 1\n",
      "Many: 1\n",
      "Try: 1\n",
      "built,: 1\n",
      "\"yarn-client\": 1\n",
      "YARN,: 1\n",
      "not: 1\n",
      "using: 2\n",
      "Example: 1\n",
      "scala>: 1\n",
      "Once: 1\n",
      "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).: 1\n",
      "Because: 1\n",
      "cluster.: 1\n",
      "name: 1\n",
      "Testing: 1\n",
      "Streaming: 1\n",
      "./bin/pyspark: 1\n",
      "SQL: 2\n",
      "through: 1\n",
      "GraphX: 1\n",
      "them,: 1\n",
      "[run: 1\n",
      "\"yarn-cluster\": 1\n",
      "the: 21\n",
      "abbreviated: 1\n",
      "set: 2\n",
      "[project: 2\n",
      "Scala: 2\n",
      "##: 8\n",
      "thread,: 1\n",
      "library: 1\n",
      "see: 1\n",
      "examples: 2\n",
      "MASTER: 1\n",
      "runs.: 1\n",
      "[Apache: 1\n",
      "Pi: 1\n",
      "instructions.: 1\n",
      "More: 1\n",
      "Python,: 2\n",
      "#: 1\n",
      "processing,: 2\n",
      "for: 11\n",
      "its: 1\n",
      "version: 1\n",
      "wiki](https://cwiki.apache.org/confluence/display/SPARK).: 1\n",
      "provides: 1\n",
      "print: 1\n",
      "Configuration: 1\n",
      "supports: 2\n",
      "command,: 2\n",
      "[params]`.: 1\n",
      "refer: 2\n",
      "available: 1\n",
      "core: 1\n",
      "run: 7\n",
      "./bin/run-example: 2\n",
      "Versions: 1\n",
      "This: 2\n",
      "Hadoop,: 2\n",
      "Documentation: 1\n",
      "use: 3\n",
      "downloaded: 1\n",
      "distributions.: 1\n",
      "Spark.: 1\n",
      "example:: 1\n",
      "`examples`: 2\n",
      "-DskipTests: 1\n",
      "Maven](http://maven.apache.org/).: 1\n",
      "[\"Building: 1\n",
      "works: 1\n",
      "package: 1\n",
      "of: 5\n",
      "changed: 1\n",
      "programming: 1\n",
      "optimized: 1\n",
      "against: 1\n",
      "site,: 1\n",
      "graph: 1\n",
      "For: 2\n",
      "or: 3\n",
      "mvn: 1\n",
      "learning,: 1\n",
      "and: 10\n",
      "contains: 1\n",
      "stream: 1\n",
      "can: 6\n",
      "overview: 1\n",
      "package.): 1\n",
      "prefer: 1\n",
      "one: 2\n",
      "documentation,: 1\n",
      "Data.: 1\n",
      "(You: 1\n",
      "Online: 1\n",
      "tools: 1\n",
      "your: 1\n",
      "threads.: 1\n",
      "Tests: 1\n",
      "fast: 1\n",
      "from: 1\n",
      "package.: 1\n",
      "APIs: 1\n",
      "SparkPi: 2\n",
      "structured: 1\n",
      "system: 1\n",
      "submit: 1\n",
      "systems.: 1\n",
      "start: 1\n",
      "params: 1\n",
      "Hadoop-supported: 1\n",
      "way: 1\n",
      "basic: 1\n",
      "README: 1\n",
      "<http://spark.apache.org/>: 1\n",
      "It: 2\n",
      "Spark](#building-spark).: 1\n",
      "engine: 1\n",
      "building: 3\n",
      "configure: 1\n",
      "on: 6\n",
      "Note: 1\n",
      "N: 1\n",
      "usage: 1\n",
      "\"local[N]\": 1\n",
      ">>>: 1\n",
      "particular: 3\n",
      "instance:: 1\n",
      "./bin/spark-shell: 1\n",
      "be: 2\n",
      "general: 2\n",
      "with: 4\n",
      "easiest: 1\n",
      "protocols: 1\n",
      "must: 1\n",
      "And: 1\n",
      "also: 5\n",
      "versions: 1\n",
      "this: 1\n",
      "project: 1\n",
      "setup: 1\n",
      "page](http://spark.apache.org/documentation.html): 1\n",
      "shell:: 2\n",
      "will: 1\n",
      "See: 1\n",
      "`./bin/run-example: 1\n",
      "guide](http://spark.apache.org/docs/latest/configuration.html): 1\n",
      "following: 2\n",
      "locally: 2\n",
      "distribution: 1\n",
      "example: 3\n",
      "are: 1\n",
      "detailed: 2\n",
      "tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).: 1\n",
      "mesos://: 1\n",
      "Please: 3\n",
      "computing: 1\n",
      "URL,: 1\n",
      "is: 6\n",
      "in: 5\n",
      "higher-level: 1\n",
      "tests: 1\n",
      "1000:: 2\n",
      "an: 3\n",
      "sample: 1\n",
      "To: 2\n",
      "at: 2\n",
      "have: 1\n",
      "1000).count(): 1\n",
      "[\"Specifying: 1\n",
      "Party: 1\n",
      "[building: 1\n",
      "You: 3\n",
      "if: 4\n",
      "Spark: 14\n",
      "different: 1\n",
      "MASTER=spark://host:7077: 1\n",
      "no: 1\n",
      "programs,: 1\n",
      "Java,: 1\n",
      "that: 3\n",
      "storage: 1\n",
      "MLlib: 1\n",
      "same: 1\n",
      "machine: 1\n",
      "application: 1\n",
      "need: 1\n",
      "other: 1\n",
      "analysis.: 1\n",
      "build: 3\n",
      "which: 2\n",
      "online: 1\n",
      "you: 4\n",
      "several: 1\n",
      "A: 1\n",
      "distribution.: 1\n",
      "About: 1\n",
      "HDFS: 1\n",
      "[Configuration: 1\n",
      "sc.parallelize(1: 1\n",
      "locally.: 1\n",
      "Hive: 2\n",
      "running: 1\n",
      "uses: 1\n",
      "a: 9\n",
      "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version): 1\n",
      "variable: 1\n",
      "The: 1\n",
      "data: 2\n",
      "class: 2\n",
      "built: 1\n",
      "Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html): 1\n",
      "Thriftserver: 1\n",
      "processing.: 1\n",
      "programs: 2\n",
      "documentation: 3\n",
      "pre-built: 1\n",
      "Alternatively,: 1\n",
      "Python: 2\n",
      "./dev/run-tests: 1\n",
      "comes: 1\n",
      "clean: 1\n",
      "<class>: 1\n",
      "spark://: 1\n",
      "first: 1\n",
      "requires: 1\n",
      "talk: 1\n",
      "latest: 1\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(os.path.join(SPARK_HOME, \"README.md\"), 1)\n",
    "linesnew = lines.filter( lambda x: len(x) > 3 )\n",
    "counts = linesnew.flatMap(lambda x: x.split(' ')).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "   print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all: 1\n",
      "help: 1\n",
      "when: 1\n",
      "automated: 1\n",
      "Hadoop: 4\n",
      "\"local\": 1\n",
      "including: 3\n",
      "computation: 1\n",
      "[\"Third: 1\n",
      "file: 1\n",
      "high-level: 1\n",
      "find: 1\n",
      "web: 1\n",
      "Shell: 2\n",
      "cluster: 2\n",
      "how: 2\n",
      "using:: 1\n",
      "Big: 1\n",
      "guidance: 3\n",
      "run:: 1\n",
      "Scala,: 1\n",
      "Running: 1\n",
      "should: 2\n",
      "environment: 1\n",
      "to: 14\n",
      "only: 1\n",
      "given.: 1\n",
      "rich: 1\n",
      "directory.: 1\n",
      "Apache: 1\n",
      "Interactive: 2\n",
      "sc.parallelize(range(1000)).count(): 1\n",
      "Building: 1\n",
      "do: 2\n",
      "guide,: 1\n",
      "return: 2\n",
      "graphs: 1\n",
      "Programs: 1\n",
      "Many: 1\n",
      "Try: 1\n",
      "built,: 1\n",
      "\"yarn-client\": 1\n",
      "YARN,: 1\n",
      "not: 1\n",
      "using: 2\n",
      "Example: 1\n",
      "scala>: 1\n",
      "Once: 1\n",
      "Spark\"](http://spark.apache.org/docs/latest/building-spark.html).: 1\n",
      "Because: 1\n",
      "cluster.: 1\n",
      "name: 1\n",
      "Testing: 1\n",
      "Streaming: 1\n",
      "./bin/pyspark: 1\n",
      "SQL: 2\n",
      "through: 1\n",
      "GraphX: 1\n",
      "them,: 1\n",
      "[run: 1\n",
      "\"yarn-cluster\": 1\n",
      "the: 21\n",
      "abbreviated: 1\n",
      "set: 2\n",
      "[project: 2\n",
      "Scala: 2\n",
      "##: 8\n",
      "thread,: 1\n",
      "library: 1\n",
      "see: 1\n",
      "examples: 2\n",
      "MASTER: 1\n",
      "runs.: 1\n",
      "[Apache: 1\n",
      "Pi: 1\n",
      "instructions.: 1\n",
      "More: 1\n",
      "Python,: 2\n",
      "#: 1\n",
      "processing,: 2\n",
      "for: 11\n",
      "its: 1\n",
      "version: 1\n",
      "wiki](https://cwiki.apache.org/confluence/display/SPARK).: 1\n",
      "provides: 1\n",
      "print: 1\n",
      "Configuration: 1\n",
      "supports: 2\n",
      "command,: 2\n",
      "[params]`.: 1\n",
      "refer: 2\n",
      "available: 1\n",
      "core: 1\n",
      "run: 7\n",
      "./bin/run-example: 2\n",
      "Versions: 1\n",
      "This: 2\n",
      "Hadoop,: 2\n",
      "Documentation: 1\n",
      "use: 3\n",
      "downloaded: 1\n",
      "distributions.: 1\n",
      "Spark.: 1\n",
      "example:: 1\n",
      "`examples`: 2\n",
      "-DskipTests: 1\n",
      "Maven](http://maven.apache.org/).: 1\n",
      "[\"Building: 1\n",
      "works: 1\n",
      "package: 1\n",
      "of: 5\n",
      "changed: 1\n",
      "programming: 1\n",
      "optimized: 1\n",
      "against: 1\n",
      "site,: 1\n",
      "graph: 1\n",
      "For: 2\n",
      "or: 3\n",
      "mvn: 1\n",
      "learning,: 1\n",
      "and: 10\n",
      "contains: 1\n",
      "stream: 1\n",
      "can: 6\n",
      "overview: 1\n",
      "package.): 1\n",
      "prefer: 1\n",
      "one: 2\n",
      "documentation,: 1\n",
      "Data.: 1\n",
      "(You: 1\n",
      "Online: 1\n",
      "tools: 1\n",
      "your: 1\n",
      "threads.: 1\n",
      "Tests: 1\n",
      "fast: 1\n",
      "from: 1\n",
      "package.: 1\n",
      "APIs: 1\n",
      "SparkPi: 2\n",
      "structured: 1\n",
      "system: 1\n",
      "submit: 1\n",
      "systems.: 1\n",
      "start: 1\n",
      "params: 1\n",
      "Hadoop-supported: 1\n",
      "way: 1\n",
      "basic: 1\n",
      "README: 1\n",
      "<http://spark.apache.org/>: 1\n",
      "It: 2\n",
      "Spark](#building-spark).: 1\n",
      "engine: 1\n",
      "building: 3\n",
      "configure: 1\n",
      "on: 6\n",
      "Note: 1\n",
      "N: 1\n",
      "usage: 1\n",
      "\"local[N]\": 1\n",
      ">>>: 1\n",
      "particular: 3\n",
      "instance:: 1\n",
      "./bin/spark-shell: 1\n",
      "be: 2\n",
      "general: 2\n",
      "with: 4\n",
      "easiest: 1\n",
      "protocols: 1\n",
      "must: 1\n",
      "And: 1\n",
      "also: 5\n",
      "versions: 1\n",
      "this: 1\n",
      "project: 1\n",
      "setup: 1\n",
      "page](http://spark.apache.org/documentation.html): 1\n",
      "shell:: 2\n",
      "will: 1\n",
      "See: 1\n",
      "`./bin/run-example: 1\n",
      "guide](http://spark.apache.org/docs/latest/configuration.html): 1\n",
      "following: 2\n",
      "locally: 2\n",
      "distribution: 1\n",
      "example: 3\n",
      "are: 1\n",
      "detailed: 2\n",
      "tests](https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark#ContributingtoSpark-AutomatedTesting).: 1\n",
      "mesos://: 1\n",
      "Please: 3\n",
      "computing: 1\n",
      "URL,: 1\n",
      "is: 6\n",
      "in: 5\n",
      "higher-level: 1\n",
      "tests: 1\n",
      "1000:: 2\n",
      "an: 3\n",
      "sample: 1\n",
      "To: 2\n",
      "at: 2\n",
      "have: 1\n",
      "1000).count(): 1\n",
      "[\"Specifying: 1\n",
      "Party: 1\n",
      "[building: 1\n",
      "You: 3\n",
      "if: 4\n",
      "Spark: 14\n",
      "different: 1\n",
      "MASTER=spark://host:7077: 1\n",
      "no: 1\n",
      "programs,: 1\n",
      "Java,: 1\n",
      "that: 3\n",
      "storage: 1\n",
      "MLlib: 1\n",
      "same: 1\n",
      "machine: 1\n",
      "application: 1\n",
      "need: 1\n",
      "other: 1\n",
      "analysis.: 1\n",
      "build: 3\n",
      "which: 2\n",
      "online: 1\n",
      "you: 4\n",
      "several: 1\n",
      "A: 1\n",
      "distribution.: 1\n",
      "About: 1\n",
      "HDFS: 1\n",
      "[Configuration: 1\n",
      "sc.parallelize(1: 1\n",
      "locally.: 1\n",
      "Hive: 2\n",
      "running: 1\n",
      "uses: 1\n",
      "a: 9\n",
      "Version\"](http://spark.apache.org/docs/latest/building-spark.html#specifying-the-hadoop-version): 1\n",
      "variable: 1\n",
      "The: 1\n",
      "data: 2\n",
      "class: 2\n",
      "built: 1\n",
      "Distributions\"](http://spark.apache.org/docs/latest/hadoop-third-party-distributions.html): 1\n",
      "Thriftserver: 1\n",
      "processing.: 1\n",
      "programs: 2\n",
      "documentation: 3\n",
      "pre-built: 1\n",
      "Alternatively,: 1\n",
      "Python: 2\n",
      "./dev/run-tests: 1\n",
      "comes: 1\n",
      "clean: 1\n",
      "<class>: 1\n",
      "spark://: 1\n",
      "first: 1\n",
      "requires: 1\n",
      "talk: 1\n",
      "latest: 1\n"
     ]
    }
   ],
   "source": [
    "counts = lines.flatMap(lambda x: x.strip().split(' ')).filter(lambda x:len(x)>0).map(lambda x: (x, 1)).reduceByKey(lambda x,y:x+y)\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "   print(\"%s: %i\" % (word, count))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['1', '2', '3', '4', '5'], ['6', '7', '8', '9', '0']]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = sc.parallelize(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'],2)\n",
    "data.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1', '2', '3', '4', '5', '6', '7', '8', '9', '0']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0')]\n",
      "[['0'], ['1'], ['2'], ['3'], ['4'], ['5'], ['6'], ['7'], ['8'], ['9']]\n",
      "[('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0')]\n",
      "[['2', '4', '6', '8', '0'], ['1', '3', '5', '7', '9']]\n"
     ]
    }
   ],
   "source": [
    "# Custom Paritioner 1\n",
    "data = sc.parallelize(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'],2).map(lambda x: (x,x))\n",
    "print data.collect()\n",
    "c = data.count()\n",
    "wp = data.partitionBy(c,lambda k: int(k))\n",
    "print wp.map(lambda t: t[0]).glom().collect()\n",
    "# Custom Paritioner 2\n",
    "data = sc.parallelize(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'],2).map(lambda x: (x,x))\n",
    "print data.collect()\n",
    "wp = data.partitionBy(2,lambda k: int(k)%2)\n",
    "print wp.map(lambda t: t[0]).glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[54] at RDD at PythonRDD.scala:43"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wp.glom()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', '1'), ('2', '2'), ('3', '3'), ('4', '4'), ('5', '5'), ('6', '6'), ('7', '7'), ('8', '8'), ('9', '9'), ('0', '0')]\n",
      "[[('0', '0')], [('1', '1')], [('2', '2')], [('3', '3')], [('4', '4')], [('5', '5')], [('6', '6')], [('7', '7')], [('8', '8')], [('9', '9')]]\n"
     ]
    }
   ],
   "source": [
    "data = sc.parallelize(['1', '2', '3', '4', '5', '6', '7', '8', '9', '0'],2).map(lambda x: (x,x))\n",
    "print data.collect()\n",
    "c = data.count()\n",
    "wp = data.partitionBy(c,lambda k: int(k))\n",
    "print wp.glom().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rdd= sc.parallelize([(1,2), (3,4), (3,6)]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 10)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 'a'), (1, 'b'), (2, 'c')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd2 = sc.parallelize([(1,'a'), (2,'c'), (1,'b')])\n",
    "rdd2.sortByKey().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dispatch(seq):\n",
    "    left_origin = []\n",
    "    right_origin = []\n",
    "    for (n, v) in seq:\n",
    "        if n == 1:\n",
    "            left_origin.append(v)\n",
    "        elif n == 2:\n",
    "            right_origin.append(v)\n",
    "    return [ v for v in left_origin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 4), ('b', 2)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([(\"a\", 1), (\"b\", 1), (\"a\", 1)])\n",
    "from operator import add\n",
    "rdd.foldByKey(1, add).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
